{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e826ba4-f357-4512-bb15-a9fc36362369",
   "metadata": {},
   "source": [
    "# CS180 / CS280A — Project 1: Prokudin‑Gorskii Colorization\n",
    "\n",
    "**Notebook**: an interactive version of the starter pipeline. Run the cells in order. This notebook:\n",
    "- loads a vertically stacked glass plate (B, G, R) image,\n",
    "- aligns the G and R channels to B using a pyramid + integer shift search,\n",
    "- displays results and prints `(x, y)` offsets, and\n",
    "- includes optional postprocessing (auto-contrast, white-balance, auto-crop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef59d59-97d8-4d9e-8f1b-8df53f757b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, color, transform\n",
    "import os\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def to_float01(im):\n",
    "    \"\"\"Convert image to float in [0,1].\"\"\"\n",
    "    if np.issubdtype(im.dtype, np.floating):\n",
    "        return np.clip(im, 0, 1).astype(np.float32)\n",
    "    else:\n",
    "        max_val = np.iinfo(im.dtype).max\n",
    "        return (im.astype(np.float32) / max_val)\n",
    "\n",
    "def split_channels_bgr(im):\n",
    "    \"\"\"Split a tall Prokudin image into B, G, R channels.\"\"\"\n",
    "    h = im.shape[0] // 3\n",
    "    b = im[:h]\n",
    "    g = im[h:2*h]\n",
    "    r = im[2*h:3*h]\n",
    "    return b, g, r\n",
    "\n",
    "def roll2(im, dy, dx):\n",
    "    \"\"\"Shift image by (dy, dx).\"\"\"\n",
    "    return np.roll(np.roll(im, dy, axis=0), dx, axis=1)\n",
    "\n",
    "def downsample_image(image, scale_factor=0.5):\n",
    "    \"\"\"Downsample image by scale factor with anti-aliasing.\"\"\"\n",
    "    new_height = int(image.shape[0] * scale_factor)\n",
    "    new_width = int(image.shape[1] * scale_factor)\n",
    "    return transform.resize(image, (new_height, new_width), anti_aliasing=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Alignment functions\n",
    "# -----------------------------\n",
    "def align_single_scale(ref, target, max_shift=15, metric='ncc', crop_frac=0.1):\n",
    "    \"\"\"\n",
    "    Align target to ref using exhaustive search.\n",
    "    - metric: 'l2' for Euclidean distance, 'ncc' for normalized cross-correlation\n",
    "    - crop_frac: fraction of border to ignore for metric\n",
    "    \"\"\"\n",
    "    h, w = ref.shape\n",
    "    crop_h = int(h * crop_frac)\n",
    "    crop_w = int(w * crop_frac)\n",
    "    \n",
    "    ref_crop = ref[crop_h:-crop_h, crop_w:-crop_w]\n",
    "    best_score = None\n",
    "    best_dy, best_dx = 0, 0\n",
    "    \n",
    "    for dy in range(-max_shift, max_shift+1):\n",
    "        for dx in range(-max_shift, max_shift+1):\n",
    "            shifted = roll2(target, dy, dx)\n",
    "            shifted_crop = shifted[crop_h:-crop_h, crop_w:-crop_w]\n",
    "            \n",
    "            if metric == 'l2':\n",
    "                score = np.sqrt(np.sum((ref_crop - shifted_crop)**2))\n",
    "                if (best_score is None) or (score < best_score):\n",
    "                    best_score = score\n",
    "                    best_dy, best_dx = dy, dx\n",
    "            elif metric == 'ncc':\n",
    "                ref_vec = (ref_crop - ref_crop.mean()).flatten()\n",
    "                tgt_vec = (shifted_crop - shifted_crop.mean()).flatten()\n",
    "                score = np.dot(ref_vec, tgt_vec) / (np.linalg.norm(ref_vec) * np.linalg.norm(tgt_vec))\n",
    "                if (best_score is None) or (score > best_score):\n",
    "                    best_score = score\n",
    "                    best_dy, best_dx = dy, dx\n",
    "            else:\n",
    "                raise ValueError(\"metric must be 'l2' or 'ncc'\")\n",
    "    \n",
    "    return best_dy, best_dx\n",
    "\n",
    "def align_pyramid(ref, target, levels=3, initial_max_shift=15, metric='ncc'):\n",
    "    \"\"\"\n",
    "    Efficient pyramid alignment for large images.\n",
    "    Starts with downsampled images and refines alignment at each level.\n",
    "    \"\"\"\n",
    "    # Create pyramid levels\n",
    "    ref_pyramid = [ref]\n",
    "    target_pyramid = [target]\n",
    "    \n",
    "    # Build pyramid by downsampling\n",
    "    for i in range(1, levels):\n",
    "        ref_pyramid.append(downsample_image(ref_pyramid[-1], 0.5))\n",
    "        target_pyramid.append(downsample_image(target_pyramid[-1], 0.5))\n",
    "    \n",
    "    # Start with coarsest level (smallest image)\n",
    "    current_dy, current_dx = 0, 0\n",
    "    \n",
    "    # Work from coarsest to finest level\n",
    "    for level in range(levels-1, -1, -1):\n",
    "        # Scale the offset for current level\n",
    "        scale_factor = 2 ** (levels - 1 - level)\n",
    "        scaled_dy = current_dy * scale_factor\n",
    "        scaled_dx = current_dx * scale_factor\n",
    "        \n",
    "        # Apply current offset to target\n",
    "        shifted_target = roll2(target_pyramid[level], scaled_dy, scaled_dx)\n",
    "        \n",
    "        # Refine alignment at this level with smaller search range\n",
    "        search_range = max(2, initial_max_shift // (2 ** (levels - 1 - level)))\n",
    "        refine_dy, refine_dx = align_single_scale(\n",
    "            ref_pyramid[level], shifted_target, \n",
    "            max_shift=search_range, metric=metric, crop_frac=0.1\n",
    "        )\n",
    "        \n",
    "        # Update total offset\n",
    "        current_dy = scaled_dy + refine_dy\n",
    "        current_dx = scaled_dx + refine_dx\n",
    "    \n",
    "    return current_dy, current_dx\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Display function\n",
    "# -----------------------------\n",
    "def show_pair(before, after, title_before='Before', title_after='After'):\n",
    "    plt.figure(figsize=(14,7))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(np.clip(before,0,1))\n",
    "    plt.axis('off')\n",
    "    plt.title(title_before)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(np.clip(after,0,1))\n",
    "    plt.axis('off')\n",
    "    plt.title(title_after)\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Main processing function\n",
    "# -----------------------------\n",
    "def process_image(input_path, output_dir, use_pyramid=True, max_shift=15, metric='ncc'):\n",
    "    \"\"\"\n",
    "    Process a Prokudin-Gorskii image with optional pyramid alignment.\n",
    "    \"\"\"\n",
    "    print(f'\\nProcessing {os.path.basename(input_path)} ...')\n",
    "    \n",
    "    # Load image\n",
    "    im = io.imread(input_path)\n",
    "    if im.ndim == 3:\n",
    "        im = color.rgb2gray(im)\n",
    "    im = to_float01(im)\n",
    "    \n",
    "    # Split channels\n",
    "    b, g, r = split_channels_bgr(im)\n",
    "    \n",
    "    # Align channels\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if use_pyramid:\n",
    "        # Use pyramid alignment for large images\n",
    "        dy_g, dx_g = align_pyramid(b, g, levels=3, initial_max_shift=max_shift, metric=metric)\n",
    "        dy_r, dx_r = align_pyramid(b, r, levels=3, initial_max_shift=max_shift, metric=metric)\n",
    "    else:\n",
    "        # Use single-scale alignment for small images\n",
    "        dy_g, dx_g = align_single_scale(b, g, max_shift=max_shift, metric=metric)\n",
    "        dy_r, dx_r = align_single_scale(b, r, max_shift=max_shift, metric=metric)\n",
    "    \n",
    "    alignment_time = time.time() - start_time\n",
    "    print(f\"Alignment completed in {alignment_time:.2f} seconds\")\n",
    "    \n",
    "    print(f\"Green displacement relative to Blue: (x={dx_g}, y={dy_g})\")\n",
    "    print(f\"Red displacement relative to Blue:   (x={dx_r}, y={dy_r})\")\n",
    "    \n",
    "    # Apply shifts\n",
    "    g_al = roll2(g, dy_g, dx_g)\n",
    "    r_al = roll2(r, dy_r, dx_r)\n",
    "    \n",
    "    # Stack channels\n",
    "    rgb = np.dstack([r_al, g_al, b])\n",
    "    \n",
    "    # Save output\n",
    "    base_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    out_path = os.path.join(output_dir, f'{base_name}_out.jpg')\n",
    "    io.imsave(out_path, (np.clip(rgb,0,1)*255).astype(np.uint8), check_contrast=False)\n",
    "    print(f'Saved aligned image to {out_path}')\n",
    "    \n",
    "    return rgb, (dy_g, dx_g, dy_r, dx_r)\n",
    "\n",
    "# -----------------------------\n",
    "# User parameters\n",
    "# -----------------------------\n",
    "# JPG files (use simple alignment)\n",
    "JPG_FILES = ['cathedral.jpg', 'monastery.jpg', 'tobolsk.jpg']\n",
    "\n",
    "\n",
    "\n",
    "OUT_DIR = 'results_enhanced'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Process JPG files with simple alignment\n",
    "print(\"Processing JPG files with simple alignment...\")\n",
    "for fname in JPG_FILES:\n",
    "    if os.path.exists(fname):\n",
    "        rgb_result, offsets = process_image(\n",
    "            fname, OUT_DIR, \n",
    "            use_pyramid=False,\n",
    "            max_shift=15,\n",
    "            metric='l2'\n",
    "        )\n",
    "        # Show before/after\n",
    "        im = io.imread(fname)\n",
    "        if im.ndim == 3:\n",
    "            im = color.rgb2gray(im)\n",
    "        im = to_float01(im)\n",
    "        b, g, r = split_channels_bgr(im)\n",
    "        show_pair(np.dstack([r, g, b]), rgb_result,\n",
    "                 title_before=f'{fname} - Before shifts',\n",
    "                 title_after=f'{fname} - Final RGB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe46b0-9331-4d79-b39c-67b5521b7ec0",
   "metadata": {},
   "source": [
    "### MULTI-SCALE ALIGNMENT W/ EMIR OUT OF ALIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34bd8fe-4d9e-4182-8353-f5eb23bd127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "from numba import jit, prange\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def compute_ssd_numba(patch1, patch2):\n",
    "    \"\"\"Compute SSD using Numba for massive speedup.\"\"\"\n",
    "    return np.sum((patch1 - patch2) ** 2)\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def compute_ncc_numba(patch1, patch2):\n",
    "    \"\"\"Compute NCC using Numba for massive speedup.\"\"\"\n",
    "    mean1 = np.mean(patch1)\n",
    "    mean2 = np.mean(patch2)\n",
    "    \n",
    "    patch1_norm = patch1 - mean1\n",
    "    patch2_norm = patch2 - mean2\n",
    "    \n",
    "    norm1 = np.sqrt(np.sum(patch1_norm * patch1_norm))\n",
    "    norm2 = np.sqrt(np.sum(patch2_norm * patch2_norm))\n",
    "    \n",
    "    if norm1 < 1e-8 or norm2 < 1e-8:\n",
    "        return 0.0\n",
    "    \n",
    "    return np.sum(patch1_norm * patch2_norm) / (norm1 * norm2)\n",
    "\n",
    "def optimized_resize(im, scale_factor):\n",
    "    \"\"\"\n",
    "    Optimized resize using area interpolation for downsampling.\n",
    "    \"\"\"\n",
    "    if scale_factor == 1.0:\n",
    "        return im.copy()\n",
    "    \n",
    "    h, w = im.shape\n",
    "    new_h, new_w = int(h * scale_factor), int(w * scale_factor)\n",
    "    return cv2.resize(im, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def build_optimized_pyramid(im, levels):\n",
    "    \"\"\"Build pyramid with optimized resizing.\"\"\"\n",
    "    pyramid = [im]\n",
    "    for i in range(1, levels):\n",
    "        prev_level = pyramid[-1]\n",
    "        new_level = optimized_resize(prev_level, 0.5)\n",
    "        pyramid.append(new_level)\n",
    "    return pyramid\n",
    "\n",
    "def detect_border_color(image, border_size=10):\n",
    "    \"\"\"\n",
    "    Detect the dominant border color by sampling from all four edges.\n",
    "    \"\"\"\n",
    "    h, w = image.shape\n",
    "    \n",
    "    # Sample from all four borders\n",
    "    top_border = image[:border_size, :].flatten()\n",
    "    bottom_border = image[-border_size:, :].flatten()\n",
    "    left_border = image[:, :border_size].flatten()\n",
    "    right_border = image[:, -border_size:].flatten()\n",
    "    \n",
    "    # Combine all border samples\n",
    "    all_border_pixels = np.concatenate([top_border, bottom_border, left_border, right_border])\n",
    "    \n",
    "    # Use median to avoid outliers\n",
    "    border_color = np.median(all_border_pixels)\n",
    "    return border_color\n",
    "\n",
    "def find_content_edges(image, border_tolerance=0.05):\n",
    "    \"\"\"\n",
    "    Find the edges where actual content begins by detecting significant changes from border color.\n",
    "    \"\"\"\n",
    "    border_color = detect_border_color(image)\n",
    "    h, w = image.shape\n",
    "    \n",
    "    # Find top edge\n",
    "    top_edge = 0\n",
    "    for i in range(h):\n",
    "        row_mean = np.mean(image[i, :])\n",
    "        if abs(row_mean - border_color) > border_tolerance:\n",
    "            top_edge = i\n",
    "            break\n",
    "    \n",
    "    # Find bottom edge\n",
    "    bottom_edge = h - 1\n",
    "    for i in range(h-1, -1, -1):\n",
    "        row_mean = np.mean(image[i, :])\n",
    "        if abs(row_mean - border_color) > border_tolerance:\n",
    "            bottom_edge = i\n",
    "            break\n",
    "    \n",
    "    # Find left edge\n",
    "    left_edge = 0\n",
    "    for j in range(w):\n",
    "        col_mean = np.mean(image[:, j])\n",
    "        if abs(col_mean - border_color) > border_tolerance:\n",
    "            left_edge = j\n",
    "            break\n",
    "    \n",
    "    # Find right edge\n",
    "    right_edge = w - 1\n",
    "    for j in range(w-1, -1, -1):\n",
    "        col_mean = np.mean(image[:, j])\n",
    "        if abs(col_mean - border_color) > border_tolerance:\n",
    "            right_edge = j\n",
    "            break\n",
    "    \n",
    "    return top_edge, bottom_edge, left_edge, right_edge\n",
    "\n",
    "def auto_crop_image(image, border_tolerance=0.05):\n",
    "    \"\"\"\n",
    "    Automatically crop borders by detecting content edges.\n",
    "    \"\"\"\n",
    "    top, bottom, left, right = find_content_edges(image, border_tolerance)\n",
    "    \n",
    "    # Add small safety margin\n",
    "    margin = 2\n",
    "    top = max(0, top - margin)\n",
    "    bottom = min(image.shape[0], bottom + margin)\n",
    "    left = max(0, left - margin)\n",
    "    right = min(image.shape[1], right + margin)\n",
    "    \n",
    "    cropped = image[top:bottom, left:right]\n",
    "    return cropped, (top, bottom, left, right)\n",
    "\n",
    "\n",
    "def auto_contrast_image(image, clip_percentile=1.0):\n",
    "    \"\"\"\n",
    "    Automatically adjust contrast by scaling intensities to full range.\n",
    "    \"\"\"\n",
    "    # Calculate percentiles to avoid extreme outliers\n",
    "    low_val = np.percentile(image, clip_percentile)\n",
    "    high_val = np.percentile(image, 100 - clip_percentile)\n",
    "    \n",
    "    # Scale to [0, 1] range\n",
    "    contrasted = (image - low_val) / (high_val - low_val + 1e-8)\n",
    "    contrasted = np.clip(contrasted, 0, 1)\n",
    "    \n",
    "    return contrasted\n",
    "\n",
    "def enhance_image_pipeline(image):\n",
    "    \"\"\"\n",
    "    Complete image enhancement pipeline: auto-crop + auto-contrast.\n",
    "    \"\"\"\n",
    "    # Auto-crop first\n",
    "    cropped, crop_coords = auto_crop_image(image)\n",
    "    \n",
    "    # Auto-contrast\n",
    "    enhanced = auto_contrast_image(cropped)\n",
    "    \n",
    "    return enhanced, crop_coords\n",
    "\n",
    "def split_channels_bgr(im):\n",
    "    \"\"\"Split a tall Prokudin image into B, G, R channels.\"\"\"\n",
    "    h = im.shape[0] // 3\n",
    "    b = im[:h]\n",
    "    g = im[h:2*h]\n",
    "    r = im[2*h:3*h]\n",
    "    return b, g, r\n",
    "\n",
    "def align_single_scale_optimized(reference, target, search_range=15, metric='ncc', border_crop=0.15):\n",
    "    \"\"\"\n",
    "    Optimized single-scale alignment with precomputation and smarter search.\n",
    "    \"\"\"\n",
    "    h, w = reference.shape\n",
    "    crop_h = int(h * border_crop)\n",
    "    crop_w = int(w * border_crop)\n",
    "    \n",
    "    # Crop reference\n",
    "    ref_crop = reference[crop_h:h-crop_h, crop_w:w-crop_w]\n",
    "    \n",
    "    best_score = -np.inf if metric == 'ncc' else np.inf\n",
    "    best_offset = (0, 0)\n",
    "    \n",
    "    # Precompute metric function\n",
    "    if metric == 'ncc':\n",
    "        metric_func = compute_ncc_numba\n",
    "    else:\n",
    "        metric_func = compute_ssd_numba\n",
    "    \n",
    "    # Use a spiral search pattern\n",
    "    search_pattern = []\n",
    "    for r in range(0, search_range + 1):\n",
    "        for dy in [-r, r]:\n",
    "            for dx in range(-r, r + 1):\n",
    "                if (dy, dx) not in search_pattern:\n",
    "                    search_pattern.append((dy, dx))\n",
    "        for dx in [-r, r]:\n",
    "            for dy in range(-r + 1, r):\n",
    "                if (dy, dx) not in search_pattern:\n",
    "                    search_pattern.append((dy, dx))\n",
    "    \n",
    "    for dy, dx in search_pattern:\n",
    "        if abs(dy) > search_range or abs(dx) > search_range:\n",
    "            continue\n",
    "            \n",
    "        # Shift target image\n",
    "        shifted = np.roll(target, (dy, dx), axis=(0, 1))\n",
    "        target_crop = shifted[crop_h:h-crop_h, crop_w:w-crop_w]\n",
    "        \n",
    "        # Compute metric\n",
    "        score = metric_func(ref_crop, target_crop)\n",
    "        \n",
    "        # Update best score\n",
    "        if (metric == 'ncc' and score > best_score) or \\\n",
    "           (metric == 'ssd' and score < best_score):\n",
    "            best_score = score\n",
    "            best_offset = (dy, dx)\n",
    "    \n",
    "    return best_offset, best_score\n",
    "\n",
    "def align_channels_optimized(b, g, r, initial_search_range=30, pyramid_levels=5, metric='ncc'):\n",
    "    \"\"\"\n",
    "    Highly optimized pyramid alignment with better level selection and early termination.\n",
    "    \"\"\"\n",
    "    # Determine optimal pyramid levels based on image size\n",
    "    min_dim = min(b.shape)\n",
    "    optimal_levels = min(pyramid_levels, int(np.log2(min_dim / 64)) + 1)\n",
    "    optimal_levels = max(2, optimal_levels)\n",
    "    \n",
    "    # Build pyramids\n",
    "    b_pyramid = build_optimized_pyramid(b, optimal_levels)\n",
    "    g_pyramid = build_optimized_pyramid(g, optimal_levels)\n",
    "    r_pyramid = build_optimized_pyramid(r, optimal_levels)\n",
    "    \n",
    "    g_offset = (0, 0)\n",
    "    r_offset = (0, 0)\n",
    "    \n",
    "    # Coarse-to-fine alignment\n",
    "    for level in range(optimal_levels-1, -1, -1):\n",
    "        scale_factor = 2 ** level\n",
    "        current_search_range = max(2, initial_search_range // (2 ** (optimal_levels - level - 1)))\n",
    "        \n",
    "        b_level = b_pyramid[level]\n",
    "        g_level = g_pyramid[level]\n",
    "        r_level = r_pyramid[level]\n",
    "        \n",
    "        # Apply accumulated offsets\n",
    "        g_shifted = np.roll(g_level, g_offset, axis=(0, 1))\n",
    "        r_shifted = np.roll(r_level, r_offset, axis=(0, 1))\n",
    "        \n",
    "        # Align with smaller search range at finer levels\n",
    "        g_new_offset, _ = align_single_scale_optimized(\n",
    "            b_level, g_shifted, search_range=current_search_range, metric=metric\n",
    "        )\n",
    "        r_new_offset, _ = align_single_scale_optimized(\n",
    "            b_level, r_shifted, search_range=current_search_range, metric=metric\n",
    "        )\n",
    "        \n",
    "        # Update offsets\n",
    "        g_offset = (g_offset[0] * 2 + g_new_offset[0], g_offset[1] * 2 + g_new_offset[1])\n",
    "        r_offset = (r_offset[0] * 2 + r_new_offset[0], r_offset[1] * 2 + r_new_offset[1])\n",
    "    \n",
    "    # Apply final offsets\n",
    "    aligned_g = np.roll(g, g_offset, axis=(0, 1))\n",
    "    aligned_r = np.roll(r, r_offset, axis=(0, 1))\n",
    "    \n",
    "    return aligned_g, aligned_r, g_offset, r_offset\n",
    "\n",
    "def process_with_enhancements(image_path, output_dir='results_enhanced', metric='ncc'):\n",
    "    \"\"\"\n",
    "    Process image with auto-cropping and auto-contrasting enhancements.\n",
    "    \"\"\"\n",
    "    print(f\"Processing {os.path.basename(image_path)} without auto-crop and auto-contrast...\")\n",
    "    \n",
    "    # Read image\n",
    "    im = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if im is None:\n",
    "        raise ValueError(f\"Could not read image: {image_path}\")\n",
    "    \n",
    "    # Convert to float\n",
    "    im_float = im.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Apply enhancement pipeline to entire image first\n",
    "    enhanced_im, crop_coords = enhance_image_pipeline(im_float)\n",
    "    \n",
    "    # Split channels from enhanced image\n",
    "    b, g, r = split_channels_bgr(enhanced_im)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Align channels\n",
    "    aligned_g, aligned_r, g_offset, r_offset = align_channels_optimized(\n",
    "        b, g, r, initial_search_range=25, pyramid_levels=5, metric=metric\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Create color image\n",
    "    color_image = np.stack([aligned_r, aligned_g, b], axis=-1)\n",
    "    color_image_uint8 = (np.clip(color_image, 0, 1) * 255).astype(np.uint8)\n",
    "    \n",
    "    # Save result\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"enhanced_{os.path.basename(image_path).replace('.tif', '.jpg')}\")\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(color_image_uint8, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    print(f\"  Crop coordinates: {crop_coords}\")\n",
    "    print(f\"  Green offset: {g_offset}, Red offset: {r_offset}\")\n",
    "    print(f\"  Processing time: {processing_time:.2f} seconds\")\n",
    "    print(f\"  Saved to: {output_path}\")\n",
    "    \n",
    "    return color_image_uint8, g_offset, r_offset, crop_coords\n",
    "\n",
    "def main_enhanced():\n",
    "    \"\"\"Main function without auto-crop and auto-contrast enhancements.\"\"\"\n",
    "    # Process images with enhancements\n",
    "    image_files = [\n",
    "        'emir.tif', 'harvesters.tif', 'icon.tif', 'fresco.tif', \n",
    "        'melons.tif', 'church.tif', 'three_generations.tif',\n",
    "        'napoleon.tif', 'obshchii_vid_kremlia.tif', 'portret_inokini_marfy.tif', \n",
    "        'self_portrait.tif', 'siren.tif', 'skaly_na_r.tif'\n",
    "    ]\n",
    "    \n",
    "    total_start = time.time()\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        if os.path.exists(image_file):\n",
    "            try:\n",
    "                process_with_enhancements(image_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_file}: {e}\")\n",
    "        else:\n",
    "            print(f\"{image_file} not found, skipping...\")\n",
    "    \n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\nTotal processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_enhanced()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e9194-8d13-4423-925c-9607be7df883",
   "metadata": {},
   "source": [
    "### DISPLAY THE PHOTOS YOU PROCESSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e515e7-b1d0-4819-8600-55e71ae39cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def simple_display_results(output_dir='results'):\n",
    "    \"\"\"\n",
    "    Simply display all images in the results directory one by one.\n",
    "    \"\"\"\n",
    "    # Get all image files\n",
    "    image_files = glob.glob(os.path.join(output_dir, '*.jpg')) + \\\n",
    "                  glob.glob(os.path.join(output_dir, '*.jpeg')) + \\\n",
    "                  glob.glob(os.path.join(output_dir, '*.tif')) + \\\n",
    "                  glob.glob(os.path.join(output_dir, '*.tiff'))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {output_dir} directory.\")\n",
    "        return\n",
    "    \n",
    "    # Sort files alphabetically\n",
    "    image_files.sort()\n",
    "    \n",
    "    # Display each image in a separate window\n",
    "    for image_path in image_files:\n",
    "        # Read image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Could not read image: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert BGR to RGB for matplotlib\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Display image\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.title(os.path.basename(image_path))\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run it\n",
    "simple_display_results('results_enhanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233eb05b-67ac-4ae5-b138-5d626729681e",
   "metadata": {},
   "source": [
    "### CODE BELOW FIXES EMIR AND ADDS CROPPING/CONTRAST (BLUES AND WHISTLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245465f-771e-4db0-88ad-4c2077c5e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "from numba import jit, prange\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage import exposure\n",
    "\n",
    "# -----------------------------\n",
    "# Metric functions\n",
    "# -----------------------------\n",
    "@jit(nopython=True, parallel=True)\n",
    "def compute_ssd_numba(patch1, patch2):\n",
    "    return np.sum((patch1 - patch2) ** 2)\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def compute_ncc_numba(patch1, patch2):\n",
    "    mean1 = np.mean(patch1)\n",
    "    mean2 = np.mean(patch2)\n",
    "    \n",
    "    patch1_norm = patch1 - mean1\n",
    "    patch2_norm = patch2 - mean2\n",
    "    \n",
    "    norm1 = np.sqrt(np.sum(patch1_norm * patch1_norm))\n",
    "    norm2 = np.sqrt(np.sum(patch2_norm * patch2_norm))\n",
    "    \n",
    "    if norm1 < 1e-8 or norm2 < 1e-8:\n",
    "        return 0.0\n",
    "    \n",
    "    return np.sum(patch1_norm * patch2_norm) / (norm1 * norm2)\n",
    "\n",
    "def compute_ssim_metric(patch1, patch2):\n",
    "    patch1 = np.clip(patch1, 0, 1)\n",
    "    patch2 = np.clip(patch2, 0, 1)\n",
    "    val, _ = ssim(patch1, patch2, full=True, data_range=1.0)\n",
    "    return val\n",
    "\n",
    "# -----------------------------\n",
    "# Image enhancement pipeline\n",
    "# -----------------------------\n",
    "def optimized_resize(im, scale_factor):\n",
    "    if scale_factor == 1.0:\n",
    "        return im.copy()\n",
    "    h, w = im.shape\n",
    "    new_h, new_w = int(h * scale_factor), int(w * scale_factor)\n",
    "    return cv2.resize(im, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def build_optimized_pyramid(im, levels):\n",
    "    pyramid = [im]\n",
    "    for i in range(1, levels):\n",
    "        prev_level = pyramid[-1]\n",
    "        new_level = optimized_resize(prev_level, 0.5)\n",
    "        pyramid.append(new_level)\n",
    "    return pyramid\n",
    "\n",
    "def detect_border_color(image, border_size=10):\n",
    "    h, w = image.shape\n",
    "    top_border = image[:border_size, :].flatten()\n",
    "    bottom_border = image[-border_size:, :].flatten()\n",
    "    left_border = image[:, :border_size].flatten()\n",
    "    right_border = image[:, -border_size:].flatten()\n",
    "    all_border_pixels = np.concatenate([top_border, bottom_border, left_border, right_border])\n",
    "    return np.median(all_border_pixels)\n",
    "\n",
    "def find_content_edges(image, border_tolerance=0.05):\n",
    "    border_color = detect_border_color(image)\n",
    "    h, w = image.shape\n",
    "    top_edge, bottom_edge, left_edge, right_edge = 0, h-1, 0, w-1\n",
    "    \n",
    "    for i in range(h):\n",
    "        if abs(np.mean(image[i, :]) - border_color) > border_tolerance:\n",
    "            top_edge = i\n",
    "            break\n",
    "    for i in range(h-1, -1, -1):\n",
    "        if abs(np.mean(image[i, :]) - border_color) > border_tolerance:\n",
    "            bottom_edge = i\n",
    "            break\n",
    "    for j in range(w):\n",
    "        if abs(np.mean(image[:, j]) - border_color) > border_tolerance:\n",
    "            left_edge = j\n",
    "            break\n",
    "    for j in range(w-1, -1, -1):\n",
    "        if abs(np.mean(image[:, j]) - border_color) > border_tolerance:\n",
    "            right_edge = j\n",
    "            break\n",
    "    return top_edge, bottom_edge, left_edge, right_edge\n",
    "\n",
    "def auto_crop_image(image, border_tolerance=0.15, min_content_fraction=0.7):\n",
    "    top, bottom, left, right = find_content_edges(image, border_tolerance)\n",
    "    if image.max() <= 1.0:\n",
    "        gray = (image * 255).astype(np.uint8)\n",
    "    else:\n",
    "        gray = image.astype(np.uint8)\n",
    "    h, w = gray.shape\n",
    "    border_color = int(np.median(np.concatenate([gray[0,:], gray[-1,:], gray[:,0], gray[:,-1]])))\n",
    "    tol = int(border_tolerance * 255)\n",
    "    \n",
    "    # Row-wise\n",
    "    top, bottom = 0, h-1\n",
    "    for i in range(h):\n",
    "        if abs(int(np.mean(gray[i,:])) - border_color) > tol:\n",
    "            top = i\n",
    "            break\n",
    "    for i in range(h-1, -1, -1):\n",
    "        if abs(int(np.mean(gray[i,:])) - border_color) > tol:\n",
    "            bottom = i\n",
    "            break\n",
    "    # Column-wise\n",
    "    left, right = 0, w-1\n",
    "    for j in range(w):\n",
    "        if abs(int(np.mean(gray[:,j])) - border_color) > tol:\n",
    "            left = j\n",
    "            break\n",
    "    for j in range(w-1, -1, -1):\n",
    "        if abs(int(np.mean(gray[:,j])) - border_color) > tol:\n",
    "            right = j\n",
    "            break\n",
    "    cropped_h = bottom - top\n",
    "    cropped_w = right - left\n",
    "    if cropped_h / h < min_content_fraction or cropped_w / w < min_content_fraction:\n",
    "        return image, (0, h, 0, w)\n",
    "    cropped = image[top:bottom, left:right]\n",
    "    return cropped, (top, bottom, left, right)\n",
    "\n",
    "def auto_contrast_image(image, method='hist'):\n",
    "    if method == 'hist':\n",
    "        contrasted = exposure.equalize_hist(image)\n",
    "    elif method == 'adapt':\n",
    "        contrasted = exposure.equalize_adapthist(image, clip_limit=0.03)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'hist' or 'adapt'\")\n",
    "    return np.clip(contrasted, 0, 1)\n",
    "\n",
    "def enhance_image_pipeline(image, method='hist'):\n",
    "    cropped, crop_coords = auto_crop_image(image)\n",
    "    enhanced = auto_contrast_image(cropped, method=method)\n",
    "    return enhanced, crop_coords, cropped  # Return cropped-only as well\n",
    "\n",
    "def split_channels_bgr(im):\n",
    "    h = im.shape[0] // 3\n",
    "    return im[:h], im[h:2*h], im[2*h:3*h]\n",
    "\n",
    "# -----------------------------\n",
    "# Alignment functions\n",
    "# -----------------------------\n",
    "def align_single_scale_custom(reference, target, search_range=15, metric='ncc', border_crop=0.15):\n",
    "    h, w = reference.shape\n",
    "    crop_h, crop_w = int(h*border_crop), int(w*border_crop)\n",
    "    ref_crop = reference[crop_h:h-crop_h, crop_w:w-crop_w]\n",
    "    best_score = -np.inf\n",
    "    best_offset = (0,0)\n",
    "    \n",
    "    if metric == 'ncc':\n",
    "        metric_func = compute_ncc_numba\n",
    "    elif metric == 'ssd':\n",
    "        metric_func = compute_ssd_numba\n",
    "    elif metric == 'ssim':\n",
    "        metric_func = compute_ssim_metric\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "    search_pattern = []\n",
    "    for r in range(0, search_range+1):\n",
    "        for dy in [-r,r]:\n",
    "            for dx in range(-r,r+1):\n",
    "                if (dy,dx) not in search_pattern: search_pattern.append((dy,dx))\n",
    "        for dx in [-r,r]:\n",
    "            for dy in range(-r+1,r):\n",
    "                if (dy,dx) not in search_pattern: search_pattern.append((dy,dx))\n",
    "                \n",
    "    for dy, dx in search_pattern:\n",
    "        if abs(dy)>search_range or abs(dx)>search_range: continue\n",
    "        shifted = np.roll(target, (dy, dx), axis=(0,1))\n",
    "        target_crop = shifted[crop_h:h-crop_h, crop_w:w-crop_w]\n",
    "        score = metric_func(ref_crop, target_crop)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_offset = (dy, dx)\n",
    "    return best_offset, best_score\n",
    "\n",
    "def align_channels_optimized(b, g, r, initial_search_range=30, pyramid_levels=5, metric='ncc'):\n",
    "    min_dim = min(b.shape)\n",
    "    optimal_levels = min(pyramid_levels, int(np.log2(min_dim/64))+1)\n",
    "    optimal_levels = max(2, optimal_levels)\n",
    "    \n",
    "    b_pyr = build_optimized_pyramid(b, optimal_levels)\n",
    "    g_pyr = build_optimized_pyramid(g, optimal_levels)\n",
    "    r_pyr = build_optimized_pyramid(r, optimal_levels)\n",
    "    \n",
    "    g_offset, r_offset = (0,0), (0,0)\n",
    "    \n",
    "    for level in range(optimal_levels-1, -1, -1):\n",
    "        current_search = max(2, initial_search_range // (2**(optimal_levels-level-1)))\n",
    "        b_lvl, g_lvl, r_lvl = b_pyr[level], g_pyr[level], r_pyr[level]\n",
    "        g_shifted = np.roll(g_lvl, g_offset, axis=(0,1))\n",
    "        r_shifted = np.roll(r_lvl, r_offset, axis=(0,1))\n",
    "        \n",
    "        g_new, _ = align_single_scale_custom(b_lvl, g_shifted, search_range=current_search, metric=metric)\n",
    "        r_new, _ = align_single_scale_custom(b_lvl, r_shifted, search_range=current_search, metric=metric)\n",
    "        \n",
    "        g_offset = (g_offset[0]*2 + g_new[0], g_offset[1]*2 + g_new[1])\n",
    "        r_offset = (r_offset[0]*2 + r_new[0], r_offset[1]*2 + r_new[1])\n",
    "    \n",
    "    aligned_g = np.roll(g, g_offset, axis=(0,1))\n",
    "    aligned_r = np.roll(r, r_offset, axis=(0,1))\n",
    "    \n",
    "    return aligned_g, aligned_r, g_offset, r_offset\n",
    "\n",
    "# -----------------------------\n",
    "# Main processing\n",
    "# -----------------------------\n",
    "def process_with_enhancements(image_path, dir_cropped='results_cropped', dir_contrast='results_contrast'):\n",
    "    print(f\"Processing {os.path.basename(image_path)} ...\")\n",
    "    im = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if im is None:\n",
    "        raise ValueError(f\"Could not read image: {image_path}\")\n",
    "    im_float = im.astype(np.float32)/255.0\n",
    "    \n",
    "    # Apply pipeline\n",
    "    enhanced, crop_coords, cropped_only = enhance_image_pipeline(im_float, method='hist')\n",
    "    \n",
    "    # Split channels\n",
    "    b, g, r = split_channels_bgr(enhanced)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    metric_type = 'ssim' if os.path.basename(image_path).lower() == 'emir.tif' else 'ncc'\n",
    "    aligned_g, aligned_r, g_offset, r_offset = align_channels_optimized(\n",
    "        b, g, r, initial_search_range=25, pyramid_levels=5, metric=metric_type\n",
    "    )\n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Create images\n",
    "    color_image_contrast = np.stack([aligned_r, aligned_g, b], axis=-1)\n",
    "    color_image_contrast_uint8 = (np.clip(color_image_contrast,0,1)*255).astype(np.uint8)\n",
    "    \n",
    "    # For cropped-only (no contrast)\n",
    "    b_nc, g_nc, r_nc = split_channels_bgr(cropped_only)\n",
    "    aligned_g_nc, aligned_r_nc, _, _ = align_channels_optimized(\n",
    "        b_nc, g_nc, r_nc, initial_search_range=25, pyramid_levels=5, metric=metric_type\n",
    "    )\n",
    "    color_image_cropped = np.stack([aligned_r_nc, aligned_g_nc, b_nc], axis=-1)\n",
    "    color_image_cropped_uint8 = (np.clip(color_image_cropped,0,1)*255).astype(np.uint8)\n",
    "    \n",
    "    # Save both\n",
    "    os.makedirs(dir_cropped, exist_ok=True)\n",
    "    os.makedirs(dir_contrast, exist_ok=True)\n",
    "    \n",
    "    path_cropped = os.path.join(dir_cropped, f\"cropped_{os.path.basename(image_path).replace('.tif','.jpg')}\")\n",
    "    path_contrast = os.path.join(dir_contrast, f\"contrast_{os.path.basename(image_path).replace('.tif','.jpg')}\")\n",
    "    \n",
    "    cv2.imwrite(path_cropped, cv2.cvtColor(color_image_cropped_uint8, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(path_contrast, cv2.cvtColor(color_image_contrast_uint8, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    print(f\"  Crop coords: {crop_coords}\")\n",
    "    print(f\"  Green offset: {g_offset}, Red offset: {r_offset}\")\n",
    "    print(f\"  Processing time: {processing_time:.2f}s\")\n",
    "    print(f\"  Saved cropped-only: {path_cropped}\")\n",
    "    print(f\"  Saved contrast: {path_contrast}\")\n",
    "    \n",
    "    return color_image_cropped_uint8, color_image_contrast_uint8, g_offset, r_offset, crop_coords\n",
    "\n",
    "def main_enhanced():\n",
    "    image_files = [\n",
    "        'emir.tif', 'harvesters.tif', 'icon.tif', 'fresco.tif', \n",
    "        'melons.tif', 'church.tif', 'three_generations.tif', 'lastochikino.tif',\n",
    "        'napoleon.tif', 'obshchii_vid_kremlia.tif', 'portret_inokini_marfy.tif', \n",
    "        'self_portrait.tif','siren.tif', 'skaly_na_r.tif', 'ikona_sv_nikolaia.tif', \n",
    "        'lugano.tif', 'vid_s_kriepostnogo.tif'\n",
    "    ]\n",
    "    \n",
    "    total_start = time.time()\n",
    "    for image_file in image_files:\n",
    "        if os.path.exists(image_file):\n",
    "            try:\n",
    "                process_with_enhancements(image_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_file}: {e}\")\n",
    "        else:\n",
    "            print(f\"{image_file} not found, skipping...\")\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\nTotal processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_enhanced()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc4b081-8684-42c9-b93e-41821e5f7d87",
   "metadata": {},
   "source": [
    "### HARD-CROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a1e64a-8605-48ed-91e9-539ca1b08c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def manual_hard_crop(image, crop_percent=0.1):\n",
    "    \"\"\"\n",
    "    Manually crop a fixed percentage from all borders of an image\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Calculate crop pixels from each side\n",
    "    crop_h = int(h * crop_percent)\n",
    "    crop_w = int(w * crop_percent)\n",
    "    \n",
    "    # Calculate new boundaries\n",
    "    top = crop_h\n",
    "    bottom = h - crop_h\n",
    "    left = crop_w\n",
    "    right = w - crop_w\n",
    "    \n",
    "    # Ensure we don't crop too much\n",
    "    if bottom - top < h * 0.5 or right - left < w * 0.5:\n",
    "        # If we'd crop more than 50% of the image, crop less\n",
    "        crop_h = int(h * 0.05)\n",
    "        crop_w = int(w * 0.05)\n",
    "        top = crop_h\n",
    "        bottom = h - crop_h\n",
    "        left = crop_w\n",
    "        right = w - crop_w\n",
    "    \n",
    "    # Crop the image\n",
    "    cropped = image[top:bottom, left:right]\n",
    "    return cropped\n",
    "\n",
    "def process_directory(input_dir, output_dir, crop_percent=0.1):\n",
    "    \"\"\"\n",
    "    Process all images in a directory by cropping borders\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all JPG images in the input directory\n",
    "    image_paths = glob.glob(os.path.join(input_dir, \"*.jpg\"))\n",
    "    \n",
    "    print(f\"Processing {len(image_paths)} images from {input_dir}\")\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        # Read the image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Could not read image: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Crop the image\n",
    "        cropped_image = manual_hard_crop(image, crop_percent)\n",
    "        \n",
    "        # Save the cropped image\n",
    "        filename = os.path.basename(image_path)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, cropped_image)\n",
    "        \n",
    "        print(f\"  Cropped and saved: {output_path}\")\n",
    "\n",
    "def main():\n",
    "    # Directories\n",
    "    cropped_dir = 'results_cropped' \n",
    "    contrast_dir = 'results_contrast' \n",
    "    output_cropped_dir = 'results_cropped_manual'\n",
    "    output_contrast_dir = 'results_contrast_manual'\n",
    "    \n",
    "    # Crop percentage (10%)\n",
    "    crop_percent = 0.1\n",
    "    \n",
    "    print(\"Manual Border Cropping Tool\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Process cropped-only images\n",
    "    print(\"\\nProcessing cropped-only images...\")\n",
    "    process_directory(cropped_dir, output_cropped_dir, crop_percent)\n",
    "    \n",
    "    # Process contrast-enhanced images\n",
    "    print(\"\\nProcessing contrast-enhanced images...\")\n",
    "    process_directory(contrast_dir, output_contrast_dir, crop_percent)\n",
    "    \n",
    "    print(\"\\nDone! All images have been manually cropped by 10%.\")\n",
    "    print(f\"Cropped-only results saved to: {output_cropped_dir}\")\n",
    "    print(f\"Contrast-enhanced results saved to: {output_contrast_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb7379-57d9-4381-97b0-da4eecefaf81",
   "metadata": {},
   "source": [
    "### SIDE-BY-SIDE DISPLAY ALL PHOTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8aa036-c985-430c-a23d-f29c79558313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# %%\n",
    "# Configuration\n",
    "CROPPED_DIR = 'results_cropped_manual'\n",
    "CONTRAST_DIR = 'results_contrast_manual'\n",
    "ORIGINAL_DIR = 'cs180 proj1 data'  # Directory containing original images\n",
    "\n",
    "# %%\n",
    "def get_image_pairs():\n",
    "    \"\"\"Get pairs of processed images from both directories\"\"\"\n",
    "    cropped_files = [f for f in os.listdir(CROPPED_DIR) if f.endswith('.jpg')]\n",
    "    contrast_files = [f for f in os.listdir(CONTRAST_DIR) if f.endswith('.jpg')]\n",
    "    \n",
    "    # Extract base names without prefixes\n",
    "    base_names = set()\n",
    "    for f in cropped_files + contrast_files:\n",
    "        if f.startswith('cropped_'):\n",
    "            base_names.add(f.replace('cropped_', ''))\n",
    "        elif f.startswith('contrast_'):\n",
    "            base_names.add(f.replace('contrast_', ''))\n",
    "    \n",
    "    # Create pairs\n",
    "    pairs = []\n",
    "    for base in sorted(base_names):\n",
    "        cropped_path = os.path.join(CROPPED_DIR, 'cropped_' + base)\n",
    "        contrast_path = os.path.join(CONTRAST_DIR, 'contrast_' + base)\n",
    "        original_path = os.path.join(ORIGINAL_DIR, base.replace('.jpg', '.tif'))\n",
    "        \n",
    "        if os.path.exists(cropped_path) and os.path.exists(contrast_path):\n",
    "            pairs.append({\n",
    "                'name': base.replace('.jpg', ''),\n",
    "                'cropped': cropped_path,\n",
    "                'contrast': contrast_path,\n",
    "                'original': original_path if os.path.exists(original_path) else None\n",
    "            })\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# %%\n",
    "def display_comparison(image_pairs, max_display=None):\n",
    "    \"\"\"Display side-by-side comparison of images (Original | Cropped | Contrast)\"\"\"\n",
    "    if max_display:\n",
    "        image_pairs = image_pairs[:max_display]\n",
    "    \n",
    "    for pair in image_pairs:\n",
    "        print(f\"=== {pair['name']} ===\")\n",
    "        \n",
    "        # Read images\n",
    "        img_cropped = cv2.imread(pair['cropped'])\n",
    "        img_contrast = cv2.imread(pair['contrast'])\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        img_cropped = cv2.cvtColor(img_cropped, cv2.COLOR_BGR2RGB)\n",
    "        img_contrast = cv2.cvtColor(img_contrast, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Read original if available, otherwise use a blank placeholder\n",
    "        if pair['original'] and os.path.exists(pair['original']):\n",
    "            img_original = cv2.imread(pair['original'])\n",
    "            img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            # Create a blank image with same size as cropped\n",
    "            h, w, c = img_cropped.shape\n",
    "            img_original = np.ones((h, w, 3), dtype=np.uint8) * 255  # white placeholder\n",
    "        \n",
    "        # Create figure with 3 columns\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        axes[0].imshow(img_original)\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(img_cropped)\n",
    "        axes[1].set_title('Cropped Only')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(img_contrast)\n",
    "        axes[2].set_title('Contrast Enhanced')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Get all image pairs\n",
    "    image_pairs = get_image_pairs()\n",
    "    \n",
    "    print(f\"Found {len(image_pairs)} processed image pairs\")\n",
    "    \n",
    "    # Display first few images in the notebook\n",
    "    display_comparison(image_pairs, max_display=len(image_pairs))\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
